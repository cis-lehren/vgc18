{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "We evaluate three methods of Twitter sentiment classification.\n",
    "    1. Rule based method\n",
    "    2. SVM with bag-of-words features\n",
    "    3. SVM with word embeddings\n",
    "    \n",
    "### Dataset\n",
    "Tweets are from SemEval 2016 (Cleaned by Jeremy Barnes):\n",
    "https://github.com/jbarnesspain/domain_blse/tree/master/datasets/semeval_2016\n",
    "\n",
    "### Requirement\n",
    "Python2  \n",
    "NumPy  \n",
    "scikit-learn  \n",
    "NLTK  \n",
    "overrides  \n",
    "tqdm  \n",
    "\n",
    "### Compatibility\n",
    "All codes are written in Python2 but should be compatible with Python3. Let me know if you meet compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from sys import exit\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from collections import Counter\n",
    "from svms import svmclsbinary\n",
    "from overrides import overrides\n",
    "\n",
    "from helpers import word2vec\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(object):\n",
    "    \"\"\" Our base class for Twitter sentiment classifiers\n",
    "    Attributes:\n",
    "        pos_sents: positive sentiment tweets.\n",
    "        neg_sents: negataive sentiment tweets.\n",
    "    \n",
    "    Classmethods:\n",
    "        readlines: simply read lines in corpus.\n",
    "        mynumbers: compute classification results given predictions and gold labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_sents, neg_sents):\n",
    "        self.pos_sents = SentimentClassifier.readlines(pos_sents)\n",
    "        self.neg_sents = SentimentClassifier.readlines(neg_sents)\n",
    "\n",
    "    @staticmethod\n",
    "    def readlines(myfile):\n",
    "        with open(myfile, \"r\") as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "\n",
    "    @staticmethod\n",
    "    def mynumbers(pred, y):\n",
    "        recall = recall_score(y, pred)\n",
    "        precision = precision_score(y, pred)\n",
    "        f1 = f1_score(y, pred)\n",
    "        acc = accuracy_score(y, pred)\n",
    "        return (recall, precision, f1, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ruleSentimentClassifier(SentimentClassifier):\n",
    "    \"\"\" Sentiment classifier using rule and lexicon based method. \n",
    "    We use implementations from VADER.\n",
    "    Attributes: vader -- vader classifier\n",
    "    \n",
    "    Methods:\n",
    "    compute_score: given positive and negative tweets, compute their \n",
    "    sentiment scores. \n",
    "    \"\"\"\n",
    "    def __init__(self, pos_sents, neg_sents):\n",
    "        super(ruleSentimentClassifier, self).__init__(pos_sents, neg_sents)\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        print (\"Finish reading sentences and intialize VADER ...\")\n",
    "        self.compute_score()\n",
    "\n",
    "    def compute_score(self):\n",
    "        \"\"\"\n",
    "        Compund score >= 0.05: positive\n",
    "        Compund score <= -0.05: negative\n",
    "        Interpretations of the score: \n",
    "        https://stackoverflow.com/questions/40325980/how-is-the-vader-compound-polarity-score-calculated-in-python-nltk\n",
    "        \"\"\"\n",
    "        retriver = lambda x: self.vader.polarity_scores(x)[\"compound\"]\n",
    "        _pos_preds = map(lambda x: int(x>=0.05), map(retriver, self.pos_sents))\n",
    "        _neg_preds = map(lambda x: int(x<=-0.05), map(retriver, self.neg_sents))\n",
    "        y = [1] * len(_pos_preds) + [0] * len(_neg_preds)\n",
    "        result = ruleSentimentClassifier.mynumbers(_pos_preds+_neg_preds, y)\n",
    "        print (\"-\" * 10 + \" Summary (rule) \" + \"-\" * 10)\n",
    "        print (\"Classification results:\")\n",
    "        print (\"F1: {:.2f}, Recall: {:.2f}, Precision: {:.2f}, Accuracy: {:.2f}\".format(\n",
    "        result[2], result[0], result[1], result[-1]))\n",
    "        print (\"Data distribution: {}\".format(dict(Counter(y))))\n",
    "        print (\"-\" * 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bowSentimentClassifier(SentimentClassifier):\n",
    "    \"\"\" Sentiment classification using SVM. Each word is represented by a \n",
    "    one-hot vector with length |V| where V is vocabulary of TRAINING tweets.\n",
    "    To represent a tweet, all word vectors are summed up.\n",
    "\n",
    "    Methods:\n",
    "        run:\n",
    "            train the classifier and test performance.\n",
    "        _compute_feature: \n",
    "            preprocess tweets -- downcasing and remove low frequent words.\n",
    "            compute vector representation of all tweets by calling _feature_lookup.\n",
    "        _feature_lookup:\n",
    "            given a tweeet, compute its vectorial representation.    \n",
    "    \"\"\"\n",
    "    def __init__(self, pos_sents, neg_sents, *args):\n",
    "        super(bowSentimentClassifier, self).__init__(pos_sents, neg_sents)\n",
    "        self._compute_feature()\n",
    "\n",
    "    def run(self, test_pos_sents, test_neg_sents):\n",
    "        \"\"\" Run an experiment. The binary SVM is imported from svms. Note that\n",
    "        to save time we set cross-validation to False. Setting docv=True to run\n",
    "        a 5-fold cross validation on the training dataset, though time consuming.\n",
    "        \"\"\"\n",
    "        test_pos_X = map(self._feature_lookup,\n",
    "                         bowSentimentClassifier.readlines(test_pos_sents))\n",
    "        test_neg_X = map(self._feature_lookup,\n",
    "                         bowSentimentClassifier.readlines(test_neg_sents))\n",
    "        testy = [1] * len(test_pos_X) + [-1] * len(test_neg_X)\n",
    "        self.svm = svmclsbinary(name=\"BOW\",\n",
    "                                X=self.pos_X+self.neg_X,\n",
    "                                y=self.y,\n",
    "                                testX=test_pos_X+test_neg_X,\n",
    "                                testy=testy,\n",
    "                                docv=False)\n",
    "\n",
    "    def _compute_feature(self):\n",
    "        corpus = \"\"\n",
    "        for s in self.pos_sents+self.neg_sents: corpus += s.lower()\n",
    "        vocab = [k for k, v in dict(Counter(corpus.split(\" \"))).iteritems() if v > 5]\n",
    "        self.vocab = {k: idx for idx, k in enumerate(vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print (\"Finish computing vocab, start computing features ...\")\n",
    "        self.pos_X = map(self._feature_lookup, self.pos_sents)\n",
    "        self.neg_X = map(self._feature_lookup, self.neg_sents)\n",
    "        self.y = [1] * len(self.pos_X) + [-1] * len(self.neg_X)\n",
    "        print (\"Finish computing features ... Ready to train SVM ...\")\n",
    "\n",
    "    def _feature_lookup(self, sentence):\n",
    "        X = []\n",
    "        words = sentence.split(\" \")\n",
    "        for w in words:\n",
    "            if w not in self.vocab: continue\n",
    "            vec = [0.] * self.vocab_size\n",
    "            vec[self.vocab[w]] = 1. # get the one-hot vector for this word\n",
    "            X.append(vec)\n",
    "        if len(X) > 1:\n",
    "            X = np.sum(X, axis=0)\n",
    "            assert len(X) == self.vocab_size\n",
    "            return X\n",
    "        elif len(X) == 1:\n",
    "            assert len(X[0]) == self.vocab_size\n",
    "            return X[0]\n",
    "        elif len(X) == 0:\n",
    "            return [0.] * self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vecSentimentClassifier(bowSentimentClassifier):\n",
    "    \"\"\" Sentiment classification using SVM. Each word its size N word embedding, computed \n",
    "    from fastText with large tweets corpora. Note that normally N << |V|.\n",
    "\n",
    "    Methods:\n",
    "        _feature_lookup:\n",
    "            Now we just need to load embeddings from the word2vec function.\n",
    "            Word embeddings are summed to preduce tweets representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_sents, neg_sents, emb_path):\n",
    "        self.word2vec = word2vec(emb_path)\n",
    "        super(vecSentimentClassifier, self).__init__(pos_sents, neg_sents)\n",
    "\n",
    "    @overrides\n",
    "    def _compute_feature(self):\n",
    "        assert self.word2vec is not None\n",
    "        self.pos_X = map(self._feature_lookup, self.pos_sents)\n",
    "        self.neg_X = map(self._feature_lookup, self.neg_sents)\n",
    "        self.y = [1] * len(self.pos_X) + [-1] * len(self.neg_X)\n",
    "        print (\"Finish computing features ... Ready to train SVM ...\")\n",
    "\n",
    "    @overrides\n",
    "    def _feature_lookup(self, sentence):\n",
    "        X = []\n",
    "        words = sentence.split(\" \")\n",
    "        for w in words:\n",
    "            if w not in self.word2vec: continue\n",
    "            X.append(self.word2vec[w])\n",
    "        if len(X) > 1:\n",
    "            return np.sum(X, axis=0)\n",
    "        elif len(X) == 1:\n",
    "            return X[0]\n",
    "        elif len(X) == 0:\n",
    "            return [0.] * 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then start to run our experiments ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"First specify our data path\"\"\"\n",
    "    train_pos_sents = \"./dataset/train_pos.txt\"\n",
    "    train_neg_sents = \"./dataset/train_neg.txt\"\n",
    "    test_pos_sents = \"./dataset/test_pos.txt\"\n",
    "    test_neg_sents = \"./dataset/test_neg.txt\"\n",
    "    emb_path = \"./embeddings/myemb.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First -- rule based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish reading sentences and intialize VADER ...\n",
      "---------- Summary (rule) ----------\n",
      "Classification results:\n",
      "F1: 0.71, Recall: 0.69, Precision: 0.72, Accuracy: 0.60\n",
      "Data distribution: {0: 2386, 1: 5619}\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "    cls = ruleSentimentClassifier(test_pos_sents, test_neg_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second -- ML based method with bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish computing vocab, start computing features ...\n",
      "Finish computing features ... Ready to train SVM ...\n"
     ]
    }
   ],
   "source": [
    "    cls = bowSentimentClassifier(train_pos_sents, train_neg_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training finished ... test the trained classifier ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training this svm for BOW...\n",
      "Word::BOW f1: 0.827435064935, recall: 0.907100907635, precision: 0.760632741382, acc: 0.734415990006. TestSize::8005, TestDist::Counter({1: 5619, -1: 2386})\n"
     ]
    }
   ],
   "source": [
    "    cls.run(test_pos_sents, test_neg_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third -- ML based method with word embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113281/113281 [00:03<00:00, 28737.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish computing features ... Ready to train SVM ...\n"
     ]
    }
   ],
   "source": [
    "    cls = vecSentimentClassifier(train_pos_sents, train_neg_sents, emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training this svm for BOW...\n",
      "Word::BOW f1: 0.860953000488, recall: 0.942160526784, precision: 0.792633627789, acc: 0.786383510306. TestSize::8005, TestDist::Counter({1: 5619, -1: 2386})\n"
     ]
    }
   ],
   "source": [
    "    cls.run(test_pos_sents, test_neg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
